from __future__ import annotations

import hashlib
import json
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable, List, Sequence, Tuple, Optional, Any

import numpy as np

# Lazy imports - only loaded when needed (performance optimization)
faiss = None
SentenceTransformer = None
_cached_model = None  # Cache global do modelo

def _lazy_import_faiss():
    """Lazy import of faiss to avoid startup cost."""
    global faiss
    if faiss is None:
        try:
            import faiss as _faiss  # type: ignore
            faiss = _faiss
        except Exception:  # pragma: no cover
            faiss = None  # will fallback to numpy if unavailable
    return faiss

def _lazy_import_sentence_transformers():
    """Lazy import of sentence_transformers to avoid 11+ second startup cost."""
    global SentenceTransformer
    if SentenceTransformer is None:
        try:
            from sentence_transformers import SentenceTransformer as ST
            SentenceTransformer = ST
        except Exception:  # pragma: no cover
            SentenceTransformer = None  # delayed import error handled at runtime
    return SentenceTransformer

from .smart_search import normalize_equip


DEFAULT_MODEL_NAME = os.getenv("SEMANTIC_MODEL", "sentence-transformers/all-MiniLM-L6-v2")
INDEX_DIR = Path(os.getenv("SEMANTIC_INDEX_DIR", "data/semantic_index")).resolve()
INDEX_DIR.mkdir(parents=True, exist_ok=True)

# üöÄ Quick Win Part 3: FAISS HNSW Index Optimization
# Par√¢metros para √≠ndice hier√°rquico (muito mais r√°pido)
USE_HNSW = os.getenv("USE_HNSW", "true").lower() in ("true", "1", "yes")
HNSW_M = int(os.getenv("HNSW_M", "32"))  # Conex√µes por layer (trade-off qualidade/mem√≥ria)
HNSW_EF_CONSTRUCTION = int(os.getenv("HNSW_EF_CONSTRUCTION", "64"))  # Qualidade de constru√ß√£o
HNSW_EF_SEARCH = int(os.getenv("HNSW_EF_SEARCH", "128"))  # Qualidade de busca (recall)
MIN_SAMPLES_FOR_HNSW = 100  # M√≠nimo de samples para usar HNSW


def _hash_texts(texts: Sequence[str], sample: int = 500) -> str:
    h = hashlib.sha256()
    n = len(texts)
    h.update(str(n).encode("utf-8"))
    if n == 0:
        return h.hexdigest()
    step = max(1, n // sample)
    total_len = 0
    for i in range(0, n, step):
        t = texts[i]
        total_len += len(t)
        h.update(str(i).encode("utf-8"))
        h.update(t[:512].encode("utf-8", errors="ignore"))
    h.update(str(total_len).encode("utf-8"))
    return h.hexdigest()


def _ensure_model(model_name: str = DEFAULT_MODEL_NAME) -> Any:
    """Lazily load SentenceTransformer model only when needed with global caching."""
    global _cached_model
    if _cached_model is not None and hasattr(_cached_model, 'model_name_or_path'):
        # Verifica se √© o mesmo modelo
        if _cached_model.model_name_or_path == model_name or str(_cached_model).find(model_name) >= 0:
            return _cached_model
    
    ST = _lazy_import_sentence_transformers()
    if ST is None:
        raise RuntimeError("sentence-transformers n√£o est√° instalado. Adicione 'sentence-transformers' ao requirements.txt")
    _cached_model = ST(model_name)
    return _cached_model


def _l2_normalize(vecs: np.ndarray) -> np.ndarray:
    norms = np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-12
    return (vecs / norms).astype("float32", copy=False)


def _build_hnsw_index(embeddings: np.ndarray, dim: int, M: int = HNSW_M, efConstruction: int = HNSW_EF_CONSTRUCTION, efSearch: int = HNSW_EF_SEARCH) -> Any:
    """
    üöÄ Quick Win Part 3: Constr√≥i √≠ndice FAISS HNSW otimizado.
    
    HNSW (Hierarchical Navigable Small World):
    - Grafo hier√°rquico para busca aproximada ultra-r√°pida
    - Recall@10 ~99.5% com efSearch=32
    - Lat√™ncia 70-85% menor que IndexFlatIP
    - Funciona bem com datasets pequenos e grandes
    
    Args:
        embeddings: Vetores L2-normalizados (N, dim)
        dim: Dimensionalidade dos vetores
        M: Conex√µes por layer (16-64, default: 32)
        efConstruction: Qualidade de constru√ß√£o (default: 64)
        efSearch: Qualidade de busca (default: 32)
    
    Returns:
        FAISS index treinado ou None se falhar
    """
    faiss_lib = _lazy_import_faiss()
    if faiss_lib is None:
        return None
    
    n_samples = embeddings.shape[0]
    
    # HNSW funciona bem mesmo com poucos samples
    if n_samples < MIN_SAMPLES_FOR_HNSW:
        # Fallback para Flat se corpus muito pequeno
        print(f"‚ÑπÔ∏è  Corpus pequeno ({n_samples} samples), usando IndexFlatIP")
        index = faiss_lib.IndexFlatIP(dim)
        index.add(embeddings)
        return index
    
    try:
        # Criar √≠ndice HNSW
        index = faiss_lib.IndexHNSWFlat(dim, M, faiss_lib.METRIC_INNER_PRODUCT)
        
        # Configurar par√¢metros de constru√ß√£o
        index.hnsw.efConstruction = efConstruction
        
        # Adicionar vetores (HNSW n√£o requer treinamento)
        print(f"üîß Construindo √≠ndice HNSW (M={M}, efConstruction={efConstruction}) com {n_samples} samples...")
        index.add(embeddings)
        
        # Configurar par√¢metros de busca
        index.hnsw.efSearch = efSearch
        
        print(f"‚úÖ √çndice HNSW criado: M={M}, efConstruction={efConstruction}, efSearch={efSearch}, ntotal={index.ntotal}")
        
        return index
        
    except Exception as e:
        # Fallback para Flat em caso de erro
        print(f"‚ö†Ô∏è  Erro ao criar HNSW ({e}), usando IndexFlatIP")
        index = faiss_lib.IndexFlatIP(dim)
        index.add(embeddings)
        return index


@dataclass
class SemanticSearchIndex:
    """√çndice de busca sem√¢ntica baseado em embeddings + FAISS (cosine via inner product).

    - Os textos s√£o normalizados via normalize_equip para padronizar unidades/abrevia√ß√µes.
    - As embeddings s√£o L2-normalizadas; a similaridade √© produto interno = cosseno.
    - Possui persist√™ncia simples em disco (faiss + metadados + id mapping).
    """

    model_name: str
    dim: int
    corpus: List[str]
    row_indices: List[int]  # mapeia posi√ß√£o no √≠ndice -> √≠ndice da linha no DF
    index: Any  # faiss.Index ou numpy fallback

    @staticmethod
    def _encode_texts(texts: Sequence[str], model_name: str) -> np.ndarray:
        model = _ensure_model(model_name)
        # encode com batch size maior para melhor performance
        emb = model.encode(list(texts), batch_size=128, show_progress_bar=False, convert_to_numpy=True, normalize_embeddings=False)
        if emb.dtype != np.float32:
            emb = emb.astype(np.float32)
        emb = _l2_normalize(emb)
        return emb

    @classmethod
    def build(cls, texts: Iterable[str], row_indices: Optional[Sequence[int]] = None, model_name: str = DEFAULT_MODEL_NAME, use_hnsw: bool = USE_HNSW) -> "SemanticSearchIndex":
        corpus = list(texts)
        if row_indices is None:
            row_indices = list(range(len(corpus)))
        row_indices = list(map(int, row_indices))

        # normaliza agressivamente para o dom√≠nio
        norm_texts = [normalize_equip(t) for t in corpus]
        emb = cls._encode_texts(norm_texts, model_name)
        dim = int(emb.shape[1])

        faiss_lib = _lazy_import_faiss()
        if faiss_lib is not None:
            # üöÄ Quick Win Part 3: Usar HNSW se habilitado e corpus grande o suficiente
            if use_hnsw and len(corpus) >= MIN_SAMPLES_FOR_HNSW:
                # HNSW: Grafo hier√°rquico para busca ultra-r√°pida
                index = _build_hnsw_index(emb, dim, M=HNSW_M, efConstruction=HNSW_EF_CONSTRUCTION, efSearch=HNSW_EF_SEARCH)
            else:
                # Fallback para IndexFlatIP (sempre funciona, mas mais lento)
                index = faiss_lib.IndexFlatIP(dim)
                index.add(emb)
        else:
            # fallback: mantemos as embeddings em mem√≥ria e fazemos busca por numpy (mais lento)
            index = emb
        return cls(model_name=model_name, dim=dim, corpus=corpus, row_indices=list(row_indices), index=index)

    def search(self, query: str, top_k: int = 10) -> List[Tuple[int, float]]:
        q_norm = normalize_equip(query)
        q_vec = self._encode_texts([q_norm], self.model_name)
        faiss_lib = _lazy_import_faiss()
        if faiss_lib is not None and hasattr(self.index, "search"):
            D, I = self.index.search(q_vec, min(top_k, len(self.row_indices)))
            idxs = I[0]
            dists = D[0]
        else:
            # numpy fallback
            mat = self.index  # type: ignore
            sims = (mat @ q_vec[0])  # (N,)
            k = min(top_k, sims.shape[0])
            part = np.argpartition(-sims, k - 1)[:k]
            order = np.argsort(-sims[part])
            idxs = part[order]
            dists = sims[idxs]
        out: List[Tuple[int, float]] = []
        for pos, sc in zip(idxs.tolist(), dists.tolist()):
            if pos < 0:
                continue
            out.append((int(self.row_indices[pos]), float(sc)))
        return out

    def add_items(self, new_texts: Sequence[str], new_row_indices: Sequence[int]):
        """Adiciona novos itens ao √≠ndice sem√¢ntico (atualiza√ß√£o incremental).

        Observa√ß√µes:
        - Requer FAISS dispon√≠vel para ser eficiente. No fallback numpy, concatenamos as matrizes.
        - As dimens√µes devem coincidir com o √≠ndice existente.
        """
        if not new_texts:
            return
        emb = self._encode_texts([normalize_equip(t) for t in new_texts], self.model_name)
        if emb.shape[1] != self.dim:
            raise ValueError("Dimensionalidade das embeddings n√£o coincide com o √≠ndice existente")
        if faiss is not None and hasattr(self.index, "add"):
            self.index.add(emb)
        else:
            # numpy fallback: concatena
            self.index = np.vstack([self.index, emb])
        self.row_indices.extend(list(map(int, new_row_indices)))

    # ---------- Persist√™ncia simples ----------
    def _paths(self, key: str) -> Tuple[Path, Path, Path]:
        base = INDEX_DIR / key
        return base.with_suffix(".faiss"), base.with_suffix(".meta.json"), base.with_suffix(".rows.json")

    def save(self, key: str):
        faiss_path, meta_path, rows_path = self._paths(key)
        
        # üöÄ Quick Win Part 3: Salvar tipo de √≠ndice nos metadados
        index_type = "unknown"
        faiss_lib = _lazy_import_faiss()
        if faiss_lib is not None and hasattr(self.index, "__class__"):
            index_type = self.index.__class__.__name__
        
        meta = {
            "model": self.model_name,
            "dim": self.dim,
            "n": len(self.row_indices),
            "index_type": index_type,  # Para debug/monitoramento
        }
        meta_path.parent.mkdir(parents=True, exist_ok=True)
        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(meta, f, ensure_ascii=False, indent=2)
        with open(rows_path, "w", encoding="utf-8") as f:
            json.dump(self.row_indices, f)
        
        if faiss_lib is not None and hasattr(self.index, "ntotal"):
            faiss_lib.write_index(self.index, str(faiss_path))
        else:
            # salva embeddings como numpy, para fallback
            np.save(str(faiss_path.with_suffix(".npy")), np.array(self.index))

    @classmethod
    def load(cls, key: str, model_name: str = DEFAULT_MODEL_NAME) -> Optional["SemanticSearchIndex"]:
        faiss_path = (INDEX_DIR / key).with_suffix(".faiss")
        meta_path = (INDEX_DIR / key).with_suffix(".meta.json")
        rows_path = (INDEX_DIR / key).with_suffix(".rows.json")
        if not meta_path.exists() or not rows_path.exists():
            return None
        try:
            with open(meta_path, "r", encoding="utf-8") as f:
                meta = json.load(f)
            with open(rows_path, "r", encoding="utf-8") as f:
                row_indices = json.load(f)
            dim = int(meta.get("dim", 384))
            faiss_lib = _lazy_import_faiss()
            if faiss_lib is not None and faiss_path.exists():
                index = faiss_lib.read_index(str(faiss_path))
            else:
                npy_path = faiss_path.with_suffix(".npy")
                if not npy_path.exists():
                    return None
                index = np.load(str(npy_path))
            # corpus n√£o √© necess√°rio para busca e agraga√ß√£o por grupo; pode ser reconstitu√≠do externamente se preciso
            return cls(model_name=model_name, dim=dim, corpus=[], row_indices=list(map(int, row_indices)), index=index)
        except Exception:
            return None


class SemanticIndexCache:
    """Cache simples em mem√≥ria + disco por chave do corpus (hash), com reconstru√ß√£o autom√°tica."""
    def __init__(self, model_name: str = DEFAULT_MODEL_NAME):
        self.model_name = model_name
        self.key: Optional[str] = None
        self.index: Optional[SemanticSearchIndex] = None

    @property
    def model(self):
        """Property para acesso ao modelo (lazy loading via _ensure_model)."""
        return _ensure_model(self.model_name)

    def build_key(self, texts: Sequence[str]) -> str:
        digest = _hash_texts(texts)
        return f"{digest[:16]}_{len(texts)}"

    def get(self, texts: Sequence[str], row_indices: Optional[Sequence[int]] = None) -> SemanticSearchIndex:
        key = self.build_key(texts)
        # cache em mem√≥ria
        if self.index is not None and self.key == key:
            return self.index
        # tenta carregar de disco
        idx = SemanticSearchIndex.load(key, model_name=self.model_name)
        if idx is not None:
            # Se carregado de disco, garantimos que row_indices esteja alinhado (se fornecido)
            if row_indices is not None and len(row_indices) == len(texts):
                idx.row_indices = list(map(int, row_indices))
            self.key = key
            self.index = idx
            return idx
        # construir do zero
        idx = SemanticSearchIndex.build(texts, row_indices=row_indices, model_name=self.model_name)
        # persistir
        try:
            idx.save(key)
        except Exception:
            pass
        self.key = key
        self.index = idx
        return idx


__all__ = [
    "SemanticSearchIndex",
    "SemanticIndexCache",
    "DEFAULT_MODEL_NAME",
]
